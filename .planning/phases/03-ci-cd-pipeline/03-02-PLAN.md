---
phase: 03-ci-cd-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - vcdecomp/requirements.txt
  - .github/workflows/validation.yml
  - vcdecomp/tests/test_validation.py
  - .planning/baselines/.gitkeep
autonomous: true

must_haves:
  truths:
    - "GitHub Actions workflow triggers on push to main and pull requests"
    - "Workflow runs pytest validation suite on self-hosted runner"
    - "Test results upload as artifacts for debugging"
    - "Test baselines are tracked in Git for regression detection"
  artifacts:
    - path: ".github/workflows/validation.yml"
      provides: "GitHub Actions workflow definition"
      min_lines: 40
      contains: "runs-on: [self-hosted, windows"
    - path: "vcdecomp/requirements.txt"
      provides: "Python dependencies including pytest plugins"
      contains: "pytest-github-actions-annotate-failures"
    - path: "vcdecomp/tests/test_validation.py"
      provides: "Pytest tests with baseline regression support"
      contains: "data_regression"
  key_links:
    - from: ".github/workflows/validation.yml"
      to: "vcdecomp/tests/test_validation.py"
      via: "pytest command execution"
      pattern: "pytest.*test_validation\\.py"
    - from: "vcdecomp/tests/test_validation.py"
      to: ".planning/baselines/"
      via: "pytest-regressions baseline storage"
      pattern: "data_regression\\.check"
---

<objective>
Implement GitHub Actions workflow that automatically runs pytest validation suite on every commit, with test result artifacts and baseline regression tracking.

Purpose: Establishes continuous validation to catch decompilation regressions immediately. Every push and PR triggers the complete decompile → compile → compare workflow from Phase 2, with test baselines stored in Git to detect when previously-passing scripts start failing.

Output: Working CI pipeline that runs on every commit, uploads test results as artifacts, and tracks regression baselines in version control.
</objective>

<execution_context>
@C:\Users\flori\source\repos\VC_Scripter\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\flori\source\repos\VC_Scripter\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@C:\Users\flori\source\repos\VC_Scripter\.planning\PROJECT.md
@C:\Users\flori\source\repos\VC_Scripter\.planning\ROADMAP.md
@C:\Users\flori\source\repos\VC_Scripter\.planning\STATE.md
@C:\Users\flori\source\repos\VC_Scripter\.planning\phases\03-ci-cd-pipeline\03-CONTEXT.md
@C:\Users\flori\source\repos\VC_Scripter\.planning\phases\03-ci-cd-pipeline\03-RESEARCH.md
@C:\Users\flori\source\repos\VC_Scripter\.planning\phases\02-test-suite-automation\02-01-SUMMARY.md

# Existing test infrastructure
@C:\Users\flori\source\repos\VC_Scripter\vcdecomp\tests\test_validation.py
@C:\Users\flori\source\repos\VC_Scripter\vcdecomp\tests\conftest.py
@C:\Users\flori\source\repos\VC_Scripter\vcdecomp\requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Add pytest CI/CD Dependencies</name>
  <files>vcdecomp/requirements.txt</files>
  <action>
  Update requirements.txt to include pytest plugins for GitHub Actions integration:

  Add these dependencies (maintaining existing PyQt6>=6.0.0):
  ```
  PyQt6>=6.0.0
  pytest>=9.0.2
  pytest-github-actions-annotate-failures>=0.2.0
  pytest-json-report>=1.5.0
  pytest-regressions>=2.5.0
  ```

  Rationale per RESEARCH.md:
  - pytest-github-actions-annotate-failures: Provides inline error annotations in PR diffs
  - pytest-json-report: Structured test results for programmatic processing
  - pytest-regressions: Baseline management for regression detection

  Keep version pins flexible (>=) to allow patch updates while ensuring minimum versions.
  </action>
  <verify>
  ```bash
  cat vcdecomp/requirements.txt
  ```
  Should contain all 4 pytest packages with version constraints.
  </verify>
  <done>requirements.txt contains pytest and 3 CI integration plugins with version pins</done>
</task>

<task type="auto">
  <name>Create GitHub Actions Workflow</name>
  <files>.github/workflows/validation.yml</files>
  <action>
  Create GitHub Actions workflow for automated validation on push and PR events:

  Create `.github/workflows/validation.yml`:

  ```yaml
  name: Decompilation Validation

  on:
    push:
      branches: [ main ]
    pull_request:
      branches: [ main ]

  permissions:
    contents: read
    checks: write

  jobs:
    validate:
      runs-on: [self-hosted, windows, x64]
      timeout-minutes: 30

      steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r vcdecomp/requirements.txt

      - name: Run validation tests
        run: |
          py -m pytest vcdecomp/tests/test_validation.py -v `
            --tb=short `
            --junit-xml=test-results.xml `
            --json-report --json-report-file=test-report.json
        continue-on-error: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_number }}
          path: |
            test-results.xml
            test-report.json
          retention-days: 30

      - name: Publish test summary
        if: always()
        uses: EnricoMi/publish-unit-test-result-action/windows@v2
        with:
          files: test-results.xml
          check_name: Validation Test Results
  ```

  Key design decisions:
  - `runs-on: [self-hosted, windows, x64]`: Targets runner from Plan 03-01
  - `continue-on-error: true`: Ensures artifacts upload even when tests fail (per RESEARCH.md Pattern 3)
  - `if: always()`: Upload artifacts on both success and failure
  - `timeout-minutes: 30`: Prevents hung jobs from blocking runner
  - `--tb=short`: Concise tracebacks for faster CI output review
  - JUnit XML + JSON report: Multiple formats for flexibility

  Reference: RESEARCH.md "Complete Validation Workflow" pattern
  </action>
  <verify>
  ```bash
  cat .github/workflows/validation.yml
  ```

  Confirm:
  1. File exists and is valid YAML
  2. Contains `runs-on: [self-hosted, windows, x64]`
  3. Uses actions/upload-artifact@v4 (not v3)
  4. Has `continue-on-error: true` on pytest step
  5. Has `if: always()` on artifact upload
  </verify>
  <done>GitHub Actions workflow exists with self-hosted runner targeting, pytest execution, and artifact upload on all outcomes</done>
</task>

<task type="auto">
  <name>Add Baseline Regression Testing</name>
  <files>vcdecomp/tests/test_validation.py, .planning/baselines/.gitkeep</files>
  <action>
  Extend test_validation.py to use pytest-regressions for baseline tracking:

  1. Add baseline test variant that stores expected outcomes:

  ```python
  @pytest.mark.parametrize("test_id,scr_path,original_c", TEST_SCRIPTS)
  def test_decompilation_validation_with_baseline(
      test_id, scr_path, original_c, validation_orchestrator, tmp_path, data_regression
  ):
      """
      Test decompilation with baseline regression detection.

      On first run: Generates baseline YAML with test outcomes
      On subsequent runs: Compares against baseline, fails if regression detected
      """
      # Decompile script
      scr = SCRFile(scr_path)
      disasm = Disassembler(scr)
      blocks = disasm.disassemble_function(scr.header.entry_point)

      ssa_blocks = build_ssa_all_blocks(blocks, scr)

      global_resolver = GlobalResolver(scr)
      includes = generate_include_block(scr)
      code = format_structured_function_named(
          scr, ssa_blocks, scr.header.entry_point, "main", global_resolver
      )

      # Write decompiled code
      decompiled_path = tmp_path / f"{test_id.replace('/', '_')}_decompiled.c"
      decompiled_path.write_text(includes + "\n\n" + code)

      # Validate
      result = validation_orchestrator.validate(scr_path, decompiled_path)

      # Store regression-relevant metrics
      baseline_data = {
          "verdict": result.verdict.value,
          "compilation_succeeded": result.compilation_succeeded,
          "bytecode_identical": result.bytecode_identical,
          "error_count": len(result.compilation_result.errors) if not result.compilation_succeeded else 0,
          "semantic_diffs": len(result.get_differences_by_category(DifferenceCategory.SEMANTIC)),
      }

      # Compare against baseline (fails if regression detected)
      data_regression.check(baseline_data)
  ```

  2. Create baseline storage directory:
  ```bash
  mkdir -p .planning/baselines
  echo "# Pytest regression baselines" > .planning/baselines/.gitkeep
  ```

  Baselines will be auto-generated in `.planning/baselines/test_validation/` on first test run.

  Reference: RESEARCH.md "Pattern 2: Baseline Storage in Git Repository"
  </action>
  <verify>
  ```bash
  # Check test file has baseline test
  grep -n "data_regression" vcdecomp/tests/test_validation.py

  # Check baseline directory exists
  ls -la .planning/baselines/
  ```

  Should show:
  - test_validation.py contains `data_regression.check(baseline_data)`
  - .planning/baselines/ directory exists
  </verify>
  <done>test_validation.py includes baseline regression test, .planning/baselines/ directory created for storing YAML baselines</done>
</task>

</tasks>

<verification>
1. Dependencies added: requirements.txt includes pytest + 3 CI plugins
2. Workflow created: .github/workflows/validation.yml exists with correct structure
3. Baseline test added: test_validation.py has data_regression test variant
4. Artifact upload configured: Workflow uploads JUnit XML and JSON reports
5. Continue-on-error set: Tests always upload artifacts even on failure
</verification>

<success_criteria>
- requirements.txt contains pytest>=9.0.2, pytest-github-actions-annotate-failures, pytest-json-report, pytest-regressions
- GitHub Actions workflow exists targeting self-hosted Windows runner
- Workflow triggers on push to main and pull_request events
- Workflow runs pytest validation suite with artifact upload
- Test artifacts (XML, JSON) upload on both success and failure via continue-on-error
- Baseline regression test added to test_validation.py using data_regression fixture
- Baseline storage directory created at .planning/baselines/
</success_criteria>

<output>
After completion, create `.planning/phases/03-ci-cd-pipeline/03-02-SUMMARY.md`
</output>
